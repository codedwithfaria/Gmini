# Docker Model Runner Configuration

# Model settings
model:
  # Default model configuration
  default:
    type: local
    path: /models/default
    format: gguf
    max_tokens: 2048
    temperature: 0.7
    
  # Additional model configurations
  models:
    - name: local-llm
      path: /models/local-llm
      format: gguf
      max_tokens: 4096
      temperature: 0.8
      
    - name: cloud-fallback
      type: cloud
      provider: docker-offload
      model_id: gpt-3.5-turbo
      max_tokens: 8192

# API Configuration
api:
  host: 0.0.0.0
  port: 8000
  openai_compatible: true
  cors_origins: ["*"]

# Resource Management
resources:
  gpu:
    enabled: true
    memory_limit: 8G
  cpu:
    threads: 4
    
# Caching
cache:
  enabled: true
  type: redis
  connection: redis://redis:6379

# Monitoring
monitoring:
  enabled: true
  metrics_port: 9090